{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis & Visualization of Produced Water Chemistry for Environmental & Agricultural Utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from wqchartpy import triangle_piper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA CLEANING & FILTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the data from the CSV files\n",
    "df1 = pd.read_csv('../data/split_1_USGSPWDBv2.3n.csv', low_memory=False)\n",
    "df2 = pd.read_csv('../data/split_2_USGSPWDBv2.3n.csv', low_memory=False)\n",
    "df3 = pd.read_csv('../data/split_3_USGSPWDBv2.3n.csv', low_memory=False)\n",
    "\n",
    "# Concatenate the dataframes\n",
    "frames = [df1, df2, df3]\n",
    "df_merged = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# Save the concatenated dataframe to a new CSV file\n",
    "df_merged.to_csv('../data/df_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to be removed\n",
    "columns_to_remove = [\n",
    "    \"IDDB\", \"SOURCE\", \"REFERENCE\", \"LATLONGAPX\", \"USGSREGION\", \"BASINCODE\", \n",
    "    \"STATECODE\", \"COUNTYCODE\", \"FIELD\", \"FIELDCODE\", \"WELLCODE\", \"TOWNRANGE\", \n",
    "    \"REGDIST\", \"LOC\", \"QUAD\", \"DAY\", \"DATECOMP\", \"DATEANALYS\", \"METHOD\", \n",
    "    \"OPERATOR\", \"PERMIT\", \"DFORM\", \"GROUP\", \"MEMBER\", \"AGECODE\", \"ERA\", \n",
    "    \"PERIOD\", \"EPOCH\", \"LAB\", \"REMARKS\", \"LITHOLOGY\", \"POROSITY\", \"TEMP\", \n",
    "    \"PRESSURE\", \"SG\", \"SPGRAV\", \"SPGRAVT\", \"RESIS\", \"RESIST\", \"PH\", \"PHT\", \n",
    "    \"EHORP\", \"COND\", \"CONDT\", \"TURBIDITY\", \"HEM\", \"MBAS\",\"TDS\",\"TDSCALC\", \"TSS\", \"CHARGEBAL\", \n",
    "    \"ACIDITY\", \"DIC\", \"DOC\", \"TOC\", \"CN\", \"BOD\", \"COD\", \"BENZENE\", \"TOLUENE\", \n",
    "    \"ETHYLBENZ\", \"XYLENE\", \"ACETATE\", \"BUTYRATE\", \"FORMATE\", \"LACTATE\", \n",
    "    \"PHENOLS\", \"PERC\", \"PROPIONATE\", \"PYRUVATE\", \"VALERATE\", \"ORGACIDS\", \n",
    "    \"Ar\", \"CH4\", \"C2H6\", \"CO2\", \"H2\", \"H2S\", \"He\", \"N2\", \"NH3\", \"O2\", \"ALPHA\", \n",
    "    \"BETA\", \"dD\", \"H3\", \"d7Li\", \"d11B\", \"d13C\", \"C14\", \"d18O\", \"d34S\", \n",
    "    \"d37Cl\", \"K40\", \"d81Br\", \"Sr87Sr86\", \"I129\", \"Rn222\", \"Ra226\", \"Ra228\", \n",
    "    \"cull_PH\", \"cull_MgCa\", \"cull_KCl\", \"cull_K5Na\", \"Ag\", \"Al\", \"As\", \"Au\", \n",
    "    \"B\", \"BO3\", \"Be\", \"Bi\", \"Cd\", \"Co\", \"Cr\", \"Cs\", \"Cu\", \"F\", \"FeS\", \"FeAl\", \n",
    "    \"FeAl2O3\", \"Hg\", \"I\", \"Mn\", \"Mo\", \"N\", \"NO2\", \"NO3\", \"NO3NO2\", \"NH4\", \n",
    "    \"TKN\", \"Ni\", \"OH\", \"P\", \"PO4\", \"Pb\", \"Rh\", \"Rb\", \"S\", \"SO3\", \"HS\", \"Sb\", \n",
    "    \"Sc\", \"Se\", \"Sn\", \"Ti\", \"Tl\", \"U\", \"V\", \"W\", \"Zn\"\n",
    "]\n",
    "\n",
    "# Remove the specified columns\n",
    "df_limited_column = df_merged.drop(columns=columns_to_remove, errors='ignore')\n",
    "\n",
    "# Display the updated merged dataframe\n",
    "df_limited_column.to_csv('../data/df_limited_column.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column \"BASIN_CATEGORY\" to df_filtered to aid in later filtering\n",
    "\n",
    "# Define the basin categories\n",
    "basin_categories = {\n",
    "    'Anadarko Basin': ['Amarillo Arch', 'Anadarko', 'Anadarko - Chautauqua Platform', 'Anadarko - Southern Oklahoma'],\n",
    "    'Appalachian Basin': ['Appalachian', 'Black Warrior'],\n",
    "    'Gulf Coast Basin': ['Arkla', 'Gulf Coast'],\n",
    "    'Oklahoma Platform Basins': ['Arkoma', 'Arkoma - Chautauqua Platform', 'Central Kansas Uplift', \n",
    "                                                'Chautauqua Platform', 'Cherokee', 'Kansas Basins', 'Nemaha Uplift', \n",
    "                                                'Sedgwick', 'Southern Oklahoma'],\n",
    "    'Fort Worth Basin': ['Bend Arch', 'Fort Worth'],\n",
    "    'Rocky Mountain Basins': ['Big Horn', 'Black Mesa', 'Denver', 'Green River', 'Paradox', 'Piceance', \n",
    "                              'Powder River', 'San Juan', 'Uinta', 'Wind River'],\n",
    "    'Illinois Basin': ['Illinois'],\n",
    "    'Michigan Basin': ['Michigan'],\n",
    "    'Permian Basin': ['Palo Duro', 'Permian'],\n",
    "    'Williston Basin': ['Williston']\n",
    "}\n",
    "\n",
    "# Create a reverse mapping from specific basin to category\n",
    "basin_to_category = {specific: category for category, specifics in basin_categories.items() for specific in specifics}\n",
    "\n",
    "df_basins_categorized = df_limited_column.copy()\n",
    "\n",
    "# Map the 'BASIN' column to a new 'BASIN_CATEGORY' column\n",
    "df_basins_categorized['BASIN_CATEGORY'] = df_basins_categorized['BASIN'].map(basin_to_category)\n",
    "\n",
    "# Drop rows that don't have a basin category (i.e., not in your list)\n",
    "df_basins_categorized = df_basins_categorized.dropna(subset=['BASIN_CATEGORY'])\n",
    "\n",
    "# Save the filtered dataframe to a new CSV file\n",
    "df_basins_categorized.to_csv('../data/df_basins_categorized.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where TDSUSGS <= 35000 (sea water to eliminate all colebed methane produced water and also the failing analyses)\n",
    "df_filtered = df_limited_column[df_limited_column['TDSUSGS'] > 35000]\n",
    "\n",
    "# Save the filtered dataframe to a new CSV file\n",
    "df_filtered.to_csv('../data/df_filtered_TDS.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajuar\\AppData\\Local\\Temp\\ipykernel_24672\\3587715086.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered[col].fillna(0, inplace=True)\n",
      "C:\\Users\\ajuar\\AppData\\Local\\Temp\\ipykernel_24672\\3587715086.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['KNa'].fillna(0, inplace=True)\n",
      "C:\\Users\\ajuar\\AppData\\Local\\Temp\\ipykernel_24672\\3587715086.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['K'].fillna(0, inplace=True)\n",
      "C:\\Users\\ajuar\\AppData\\Local\\Temp\\ipykernel_24672\\3587715086.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered.dropna(subset=['Na'], inplace=True)\n",
      "C:\\Users\\ajuar\\AppData\\Local\\Temp\\ipykernel_24672\\3587715086.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered.dropna(subset=['Cl'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Fill NaN values in 'KNa', 'K', 'Na', 'Ca', 'Cl', 'SO4', 'Mg' with zeros for calculation\n",
    "for col in ['KNa', 'K', 'Na', 'Ca', 'Cl', 'SO4', 'Mg']:\n",
    "    df_filtered[col].fillna(0, inplace=True)\n",
    "\n",
    "# First, we'll fill NaN values in 'KNa' and 'K' with zeros for the calculation.\n",
    "df_filtered['KNa'].fillna(0, inplace=True)\n",
    "df_filtered['K'].fillna(0, inplace=True)\n",
    "\n",
    "# Apply conditions to calculate 'Na'\n",
    "# If 'Na' is missing and both 'KNa' and 'K' are present, populate 'Na' with 'KNa' - 'K'\n",
    "# If 'Na' is missing and 'KNa' is present but 'K' is not, populate 'Na' with 'KNa'\n",
    "\n",
    "na_mask = df_filtered['Na'].isna()\n",
    "na_present = df_filtered['Na'] > 0\n",
    "kna_present = df_filtered['KNa'] > 0\n",
    "k_present = df_filtered['K'] > 0\n",
    "k_missing = df_filtered['K'] == 0\n",
    "\n",
    "df_filtered.loc[k_missing & na_present & kna_present, 'K'] = df_filtered['KNa'] - df_filtered['Na']\n",
    "df_filtered.loc[na_mask & kna_present & k_present, 'Na'] = df_filtered['KNa'] - df_filtered['K']\n",
    "df_filtered.loc[na_mask & kna_present & ~k_present, 'Na'] = df_filtered['KNa']\n",
    "\n",
    "# Remove rows where 'Na' is still missing\n",
    "df_filtered.dropna(subset=['Na'], inplace=True)\n",
    "\n",
    "# Remove rows where 'Cl' is missing\n",
    "df_filtered.dropna(subset=['Cl'], inplace=True)\n",
    "\n",
    "# Save the updated dataframe \n",
    "df_filtered.to_csv('../data/df_filtered_Na_Cl.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajuar\\AppData\\Local\\Temp\\ipykernel_24672\\2291718414.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered[element + '_M'] = df_filtered[element] / molar_mass\n",
      "C:\\Users\\ajuar\\AppData\\Local\\Temp\\ipykernel_24672\\2291718414.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered[element + '_M'] = df_filtered[element] / molar_mass\n",
      "C:\\Users\\ajuar\\AppData\\Local\\Temp\\ipykernel_24672\\2291718414.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered[element + '_M'] = df_filtered[element] / molar_mass\n",
      "C:\\Users\\ajuar\\AppData\\Local\\Temp\\ipykernel_24672\\2291718414.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered[element + '_M'] = df_filtered[element] / molar_mass\n",
      "C:\\Users\\ajuar\\AppData\\Local\\Temp\\ipykernel_24672\\2291718414.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered[element + '_M'] = df_filtered[element] / molar_mass\n"
     ]
    }
   ],
   "source": [
    "#To calculate the molar concentrations from concentrations given in ppm (parts per million) or mg/L,\n",
    "#these values need to be converted into moles per liter (M). The formula to convert ppm or mg/L to M is:\n",
    "\n",
    "                #Molarity (M)=Concentration (mg/L)/ Molar Mass (g/mol) \n",
    "\n",
    "#This calculation assumes that 1 ppm is equivalent to 1 mg/L. The molar mass of each element or compound (Na, Ca, Cl, SO4, and Mg) \n",
    "#is a constant value based on its atomic or molecular weight.\n",
    "\n",
    "#Apply the conditions (molar Na > molar Ca) and (molar Cl > molar SO4) and (molar Ca > molar Mg/2) which represent likely unnatural combinations\n",
    "# Convert concentrations from ppm (mg/L) to Molarity (M)\n",
    "molar_masses = {'Na': 22.99, 'Ca': 40.08, 'Cl': 35.45, 'SO4': 96.06, 'Mg': 24.305}\n",
    "for element, molar_mass in molar_masses.items():\n",
    "    df_filtered[element + '_M'] = df_filtered[element] / molar_mass\n",
    "\n",
    "# Apply the conditions (molar Na > molar Ca) and (molar Cl > molar SO4) and (molar Ca > molar Mg/2)\n",
    "condition = (df_filtered['Na_M'] > df_filtered['Ca_M']) & \\\n",
    "            (df_filtered['Cl_M'] > df_filtered['SO4_M']) & \\\n",
    "            (df_filtered['Ca_M'] > df_filtered['Mg_M'] / 2)\n",
    "\n",
    "\n",
    "df_filtered = df_filtered[condition]\n",
    "\n",
    "# Save the updated dataframe\n",
    "df_filtered.to_csv('../data/df_filtered_corrected_elemental_ratios.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows where USGS charge balance is not between -10 and +10\n",
    "df_filtered = df_filtered[df_filtered['chargebalance'].between(-10, 10)]\n",
    "\n",
    "# Save the updated dataframe\n",
    "df_filtered.to_csv('../data/df_filtered_chargebalance.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate charge balance\n",
    "df_filtered['Cations'] = (df_filtered['Na_M'] * 1) + (df_filtered['Ca_M'] * 2) + (df_filtered['Mg_M'] * 2)\n",
    "df_filtered['Anions'] = (df_filtered['Cl_M'] * 1) + (df_filtered['SO4_M'] * 2)\n",
    "df_filtered['CalculatedChargeBalance'] = ((df_filtered['Cations'] - df_filtered['Anions']) / (df_filtered['Cations'] + df_filtered['Anions'])) * 100\n",
    "\n",
    "# Flag discrepancies between calculated charge balance and existing 'chargebalance' column\n",
    "threshold = 5  # 5% threshold for discrepancy\n",
    "df_filtered['ChargeBalanceDiscrepancy'] = abs(df_filtered['CalculatedChargeBalance'] - df_filtered['chargebalance']) > threshold\n",
    "\n",
    "# Save the updated dataframe with discrepancy flags\n",
    "df_filtered.to_csv('../data/df_filtered_discrepancy_flags.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace missing CO3 with 0's\n",
    "df_filtered['CO3'].fillna(0, inplace=True)\n",
    "\n",
    "#Replace missing HCO3 with ALKHCO3 where available\n",
    "df_filtered['HCO3'].fillna(df_filtered['ALKHCO3'], inplace=True)\n",
    "\n",
    "#Calculate HCO3 if both HCO3 and ALKHCO3 are missing\n",
    "mask = df_filtered['HCO3'].isna() & df_filtered['ALKHCO3'].isna()\n",
    "df_filtered.loc[mask, 'HCO3'] = (df_filtered['Cations'] - df_filtered['Anions']) / 61.0702\n",
    "\n",
    "# Save thdf_filtered.to_csv('../data/df_filtered_estimated_HCO3.csv', index=False)e updated dataframe with estimated HCO3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANALYSIS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for a linear regression plot\n",
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "def linear_regression_plot(x, y, ylabel_):\n",
    "    # Perform linear regression\n",
    "    slope, intercept, rvalue, pvalue, stderr = linregress(x, y)\n",
    "    \n",
    "    # Generate y values based on the regression line\n",
    "    regression_line = slope * x + intercept\n",
    "    eqn = 'y = ' + str(round(slope, 2)) + 'x + ' + str(round(intercept, 2))\n",
    "    \n",
    "    # Plot the data points\n",
    "    plt.scatter(x, y)\n",
    "    \n",
    "    # Plot the regression line\n",
    "    plt.plot(x, regression_line, 'r-')\n",
    "\n",
    "    # Add labels\n",
    "    plt.xlabel('Lithium')\n",
    "    plt.ylabel(ylabel_)\n",
    "    \n",
    "    # Display the plot\n",
    "    print(f'R value: {rvalue**2}')\n",
    "    print(f'Correlation coefficient (r): {rvalue}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe for basins, lithium, and upper depth values\n",
    "df_basin_li = df_basins_categorized[['BASIN' ,'Li', 'DEPTHUPPER']]\n",
    "\n",
    "# Organize the dataframe alphebetically by basin name\n",
    "df_basin_li = df_basin_li.sort_values(by='BASIN')\n",
    "\n",
    "# Create a new csv file for df_basin_li dataframe\n",
    "df_basin_li.to_csv('../data/df_basin_li.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe removing null values from the lithium and upper depth columns\n",
    "df_basin_li_cleaned = df_basin_li.dropna(subset=['Li', 'DEPTHUPPER'])\n",
    "\n",
    "# Create a new csv for df_basin_li_cleaned dataframe\n",
    "df_basin_li_cleaned.to_csv('../data/df_basin_li_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe removing null values from the upper depth columns\n",
    "df_li = df_basin_li.dropna(subset=['DEPTHUPPER'])\n",
    "\n",
    "# Create a new csv for df_li dataframe\n",
    "df_li.to_csv('../data/df_li.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable for all unique basin names in the dataframe\n",
    "basins_unique = df_basin_li_cleaned['BASIN'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new list to store results\n",
    "basins = []\n",
    "correlations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a for loop to calculate correlation values for each basin\n",
    "for basin in basins_unique:\n",
    "    subset_df = df_basin_li_cleaned[df_basin_li_cleaned['BASIN'] == basin]\n",
    "    \n",
    "    # Check if there are enough samples for correlation calculation\n",
    "    if len(subset_df) >= 2:\n",
    "        correlation = subset_df['Li'].corr(subset_df['DEPTHUPPER'])\n",
    "        # Add correlation values to the empty correlations list\n",
    "        correlations.append(correlation)\n",
    "        # Add basin names to the empty basins list\n",
    "        basins.append(basin)\n",
    "    else:\n",
    "        correlations.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new data frame using the basins and correlations lists\n",
    "df_correlations = pd.DataFrame({'BASIN' : basins, 'CORRELATIONS' : correlations})\n",
    "# Sort alphebetically by basin\n",
    "df_correlations = df_correlations.sort_values(by='BASIN')\n",
    "# Drop duplicate rows\n",
    "df_correlations = df_correlations.drop_duplicates()\n",
    "df_correlations.to_csv('../data/df_correlations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of basins to be removed since they have no lithium values\n",
    "basins_to_remove = [\n",
    "    \"Anadarko - Chautauqua Platform\", \"Anadarko - Southern Oklahoma\", \"Arkoma - Chautauqua Platform\", \"Cherokee\", \"Kansas Basins\", \"Nemaha Uplift\"\n",
    "]\n",
    "\n",
    "# Remove the specified basins\n",
    "df_li_limited = df_li[~df_li['BASIN'].isin(basins_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the correlation values for each basin\n",
    "correlation_dict = dict(zip(df_correlations['BASIN'], df_correlations['CORRELATIONS']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BASIN</th>\n",
       "      <th>Li</th>\n",
       "      <th>DEPTHUPPER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>92033</th>\n",
       "      <td>Amarillo Arch</td>\n",
       "      <td>-1067.022728</td>\n",
       "      <td>3279.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90008</th>\n",
       "      <td>Amarillo Arch</td>\n",
       "      <td>-1088.499855</td>\n",
       "      <td>3345.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90009</th>\n",
       "      <td>Amarillo Arch</td>\n",
       "      <td>-1093.381020</td>\n",
       "      <td>3360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90011</th>\n",
       "      <td>Amarillo Arch</td>\n",
       "      <td>-1154.883703</td>\n",
       "      <td>3549.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90013</th>\n",
       "      <td>Amarillo Arch</td>\n",
       "      <td>-1089.150677</td>\n",
       "      <td>3347.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27332</th>\n",
       "      <td>Wind River</td>\n",
       "      <td>773.956707</td>\n",
       "      <td>10768.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27331</th>\n",
       "      <td>Wind River</td>\n",
       "      <td>416.447359</td>\n",
       "      <td>5794.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23080</th>\n",
       "      <td>Wind River</td>\n",
       "      <td>1231.373170</td>\n",
       "      <td>17132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108541</th>\n",
       "      <td>Wind River</td>\n",
       "      <td>377.131393</td>\n",
       "      <td>5247.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110159</th>\n",
       "      <td>Wind River</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3155.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77170 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                BASIN           Li  DEPTHUPPER\n",
       "92033   Amarillo Arch -1067.022728      3279.0\n",
       "90008   Amarillo Arch -1088.499855      3345.0\n",
       "90009   Amarillo Arch -1093.381020      3360.0\n",
       "90011   Amarillo Arch -1154.883703      3549.0\n",
       "90013   Amarillo Arch -1089.150677      3347.0\n",
       "...               ...          ...         ...\n",
       "27332      Wind River   773.956707     10768.0\n",
       "27331      Wind River   416.447359      5794.0\n",
       "23080      Wind River  1231.373170     17132.0\n",
       "108541     Wind River   377.131393      5247.0\n",
       "110159     Wind River     1.000000      3155.0\n",
       "\n",
       "[77170 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a for loop to iterate through each row in the dataframe\n",
    "for index, row in df_li_limited.iterrows():\n",
    "    basin = row['BASIN']\n",
    "\n",
    "    # Check if the Li value is null\n",
    "    if pd.isna(row['Li']):\n",
    "        # Check if the basin has a corresponding correlation value\n",
    "        if basin in correlation_dict:\n",
    "            correlation = correlation_dict[basin]\n",
    "            # Use the correlation value to fill in the null values in the Li column\n",
    "            df_li_limited.at[index, 'Li'] = row['DEPTHUPPER'] * correlation\n",
    "\n",
    "df_li_limited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_li_limited.to_csv('../data/df_li_limited.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from wqchartpy import triangle_piper\n",
    "from wqchartpy import contour_piper\n",
    "from wqchartpy import color_piper\n",
    "\n",
    "def move_wqchartpy_image_file_to_images_folder(image_file_name, image_folder_name):\n",
    "    current_directory = os.getcwd()\n",
    "    parent_directory = os.path.dirname(current_directory)\n",
    "    current_file_path = os.path.join(parent_directory, 'notebooks', image_file_name)\n",
    "    image_folder_path = os.path.join(parent_directory, image_folder_name)\n",
    "    new_file_path = os.path.join(image_folder_path, image_file_name)\n",
    "\n",
    "    # Check if the images folder exists; if not, it gets created\n",
    "    if not os.path.exists(image_folder_path):\n",
    "        os.makedirs(image_folder_path)\n",
    "\n",
    "    # Check if a file with the same name already exists in the destination folder; if so ask user what to do: override with or without deleting the newly generated image\n",
    "    if os.path.exists(new_file_path):\n",
    "        user_input = input(f'A file with the name {image_file_name} already exists in {image_folder_name}. Do you want to override it? (y/n): ').lower()\n",
    "        if user_input != 'y':\n",
    "            user_input = input(f'Do you want to delete the newly generated image file? (y/n): ').lower()\n",
    "            if user_input == 'y':\n",
    "                os.remove(current_file_path)\n",
    "                print('File deleted.')\n",
    "            else:\n",
    "                print(f'File not deleted. It is located in {current_directory}.')\n",
    "            print(f'File not moved to images folder. It is located in {current_directory}.')\n",
    "            return\n",
    "\n",
    "    # Move the file to the lower folder\n",
    "    new_file_path = os.path.join(image_folder_path, image_file_name)\n",
    "    os.rename(current_file_path, new_file_path)\n",
    "\n",
    "    print(f\"File '{image_file_name}' moved to '{image_folder_name}' folder.\")\n",
    "    \n",
    "# The below list of column names is just to document the required column names and column order to feed into wqchartpy - variable is not used; just here for reference\n",
    "columns = ['Sample','Label','Marker','Size','Color','Alpha','Ca','Mg','Na','K','HCO3','CO3','Cl','SO4']\n",
    "\n",
    "\n",
    "# Create new dataframe to match format required by wqchartpy for the contour piper plot\n",
    "df_data_wqchartpyformat = pd.DataFrame()\n",
    "\n",
    "df_data_wqchartpyformat = pd.DataFrame()\n",
    "df_data_wqchartpyformat['Sample'] = df_filtered['IDUSGS'].map(str)\n",
    "df_data_wqchartpyformat['Label'] = 'sample'       # DECISION NEEDED --- we can get fancy with how we want to group this? maybe group it by TDS high/med/low, etc? just have single group for all for now -- only a factor if we use the normal triangle_piper\n",
    "df_data_wqchartpyformat['Marker'] = 'o'           # DECISION NEEDED --- we can get fancy with how we want to identify markers? maybe group it by TDS high/med/low, etc? just have single shape for all for now -- only a factor if we use the normal triangle_piper\n",
    "df_data_wqchartpyformat['Color'] = '#FFFF00'      # DECISION NEEDED --- we can get fancy with how we want to color this? maybe group it by TDS high/med/low, etc? just have single color for all for now -- only a factor if we use the normal triangle_piper\n",
    "df_data_wqchartpyformat['Size'] = 10\n",
    "df_data_wqchartpyformat['Alpha'] = 0.6\n",
    "\n",
    "df_data_wqchartpyformat['Ca'] = df_filtered['Ca']\n",
    "df_data_wqchartpyformat['Mg'] = df_filtered['Mg']\n",
    "df_data_wqchartpyformat['Na'] = df_filtered['Na']\n",
    "df_data_wqchartpyformat['K'] = df_filtered['K']\n",
    "df_data_wqchartpyformat['HCO3'] = df_filtered['HCO3']\n",
    "df_data_wqchartpyformat['CO3'] = df_filtered['CO3']\n",
    "df_data_wqchartpyformat['Cl'] = df_filtered['Cl']\n",
    "df_data_wqchartpyformat['SO4'] = df_filtered['SO4']\n",
    "\n",
    "# Reset the index\n",
    "df_data_wqchartpyformat.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Show the df\n",
    "df_data_wqchartpyformat.head(2)\n",
    "\n",
    "image_name = 'TestPiper1-Normal'\n",
    "triangle_piper.plot(df_data_wqchartpyformat,unit='mg/L',figname=image_name,figformat='png')\n",
    "\n",
    "image_file_name = f'{image_name}.png'\n",
    "image_folder_name = 'images'\n",
    "move_wqchartpy_image_file_to_images_folder(image_file_name, image_folder_name)\n",
    "\n",
    "#image_name = 'TestPiper2-Contour'\n",
    "#contour_piper.plot(df_data_wqchartpyformat, unit='mg/L', figname=image_name, figformat='png')\n",
    "\n",
    "#image_file_name = f'{image_name}.png'\n",
    "#image_folder_name = 'images'\n",
    "#move_wqchartpy_image_file_to_images_folder(image_file_name, image_folder_name)\n",
    "\n",
    "#image_name = 'TestPiper3-ColorCoded'\n",
    "#color_piper.plot(df_data_wqchartpyformat, unit='mg/L', figname=image_name, figformat='png')\n",
    "\n",
    "#image_file_name = f'{image_name}.png'\n",
    "#image_folder_name = 'images'\n",
    "#move_wqchartpy_image_file_to_images_folder(image_file_name, image_folder_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
